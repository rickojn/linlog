
1 names loaded

creating model ...

... model created.

initialising model ....

model initialised ....

model before training:

embedding table:
token 0:	embedding: -1.255212 	 -0.760357 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 

hidden layer: 
neuron 0, biase 0.060967
	Weight 0: 0.996038  	Weight 1: 0.141729  	Weight 2: -2.124576  	Weight 3: -0.045193  
neuron 1, biase -0.748838
	Weight 0: 0.037315  	Weight 1: -1.247397  	Weight 2: -0.991119  	Weight 3: -0.548901  
neuron 2, biase 0.734800
	Weight 0: -0.583988  	Weight 1: 0.770801  	Weight 2: 0.414911  	Weight 3: 1.210563  
neuron 3, biase 0.157078
	Weight 0: 1.340327  	Weight 1: -0.180978  	Weight 2: -0.246805  	Weight 3: 0.185108  
neuron 4, biase -0.014591
	Weight 0: -0.924544  	Weight 1: -0.115292  	Weight 2: -0.328892  	Weight 3: 1.351889  

output layer: 
neuron 0, bias -1.455196
	Weight 0: -0.168911  	Weight 1: 0.573566  	Weight 2: -0.634222  	Weight 3: 0.279987  	Weight 4: 0.316312  
neuron 1, bias -0.700985
	Weight 0: 0.210182  	Weight 1: -1.367247  	Weight 2: 0.167800  	Weight 3: -1.738395  	Weight 4: -0.699830  
neuron 2, bias -0.201957
	Weight 0: -1.131888  	Weight 1: 1.506419  	Weight 2: -0.416576  	Weight 3: 1.603918  	Weight 4: 0.512526  

epoch 0 


logit 0: -1.027256 

logit 1: -1.102803 

logit 2: -0.006001 

logit 3: -1.650766 

correct index is 1

prob [0] = 0.212589

prob [1] = 0.197120

prob [2] = 0.590291

correct index is 5

prob [3] = 0.099796

prob [4] = 0.505409

prob [5] = 0.394794

correct index is 6

prob [6] = 0.018900

prob [7] = 0.945817

prob [8] = 0.035283

loss before back = 2.173975

 backwards pass ...

 0.212589  0.099796  -0.981100 

 total delta = -0.066871 

 bias [0] = -1.455196  delta = -0.022290 

 -0.802880  0.505409  0.945817 

 total delta = 0.042544 

 bias [1] = -0.700985  delta = 0.014181 

 0.590291  -0.605206  0.035283 

 total delta = 0.016218 

 bias [2] = -0.201957  delta = 0.005406 

 neuron 0, weight 0 batch 0 grad: 0.051720

 neuron 0, weight 0 batch 1 grad: -0.085842

 neuron 0, weight 0 batch 2 grad: 0.947088

weight delta[0] = 0.912966

 neuron 0, weight 1 batch 0 grad: 0.193315

 neuron 0, weight 1 batch 1 grad: 0.015269

 neuron 0, weight 1 batch 2 grad: 0.866963

weight delta[1] = 1.075546

 neuron 0, weight 2 batch 0 grad: 0.038851

 neuron 0, weight 2 batch 1 grad: 0.070555

 neuron 0, weight 2 batch 2 grad: -0.876005

weight delta[2] = -0.766598

 neuron 0, weight 3 batch 0 grad: -0.170972

 neuron 0, weight 3 batch 1 grad: -0.088079

 neuron 0, weight 3 batch 2 grad: 0.753467

weight delta[3] = 0.494416

 neuron 0, weight 4 batch 0 grad: 0.193932

 neuron 0, weight 4 batch 1 grad: 0.084203

 neuron 0, weight 4 batch 2 grad: -0.440846

weight delta[4] = -0.162712

 neuron 1, weight 0 batch 0 grad: -0.195329

 neuron 1, weight 0 batch 1 grad: -0.434737

 neuron 1, weight 0 batch 2 grad: -0.913028

weight delta[0] = -1.543094

 neuron 1, weight 1 batch 0 grad: -0.730088

 neuron 1, weight 1 batch 1 grad: 0.077328

 neuron 1, weight 1 batch 2 grad: -0.835784

weight delta[1] = -1.488543

 neuron 1, weight 2 batch 0 grad: -0.146729

 neuron 1, weight 2 batch 1 grad: 0.357322

 neuron 1, weight 2 batch 2 grad: 0.844501

weight delta[2] = 1.055094

 neuron 1, weight 3 batch 0 grad: 0.645705

 neuron 1, weight 3 batch 1 grad: -0.446068

 neuron 1, weight 3 batch 2 grad: -0.726370

weight delta[3] = -0.526733

 neuron 1, weight 4 batch 0 grad: -0.732417

 neuron 1, weight 4 batch 1 grad: 0.426438

 neuron 1, weight 4 batch 2 grad: 0.424992

weight delta[4] = 0.119013

 neuron 2, weight 0 batch 0 grad: 0.143609

 neuron 2, weight 0 batch 1 grad: 0.520579

 neuron 2, weight 0 batch 2 grad: -0.034060

weight delta[0] = 0.630128

 neuron 2, weight 1 batch 0 grad: 0.536773

 neuron 2, weight 1 batch 1 grad: -0.092597

 neuron 2, weight 1 batch 2 grad: -0.031179

weight delta[1] = 0.412997

 neuron 2, weight 2 batch 0 grad: 0.107878

 neuron 2, weight 2 batch 1 grad: -0.427877

 neuron 2, weight 2 batch 2 grad: 0.031504

weight delta[2] = -0.288496

 neuron 2, weight 3 batch 0 grad: -0.474733

 neuron 2, weight 3 batch 1 grad: 0.534147

 neuron 2, weight 3 batch 2 grad: -0.027097

weight delta[3] = 0.032317

 neuron 2, weight 4 batch 0 grad: 0.538486

 neuron 2, weight 4 batch 1 grad: -0.510641

 neuron 2, weight 4 batch 2 grad: 0.015854

weight delta[4] = 0.043699

 0.212589  0.099796  -0.981100 

 total delta = -0.066871 

 bias [0] = -1.432906  delta = -0.022290 

 -0.802880  0.505409  0.945817 

 total delta = 0.042544 

 bias [1] = -0.715166  delta = 0.014181 

 0.590291  -0.605206  0.035283 

 total delta = 0.016218 

 bias [2] = -0.207363  delta = 0.005406 

embedding table:
token 0:	embedding: -1.215843 	 -0.794858 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 

hidden layer: 
neuron 0, biase 0.060967
	Weight 0: 1.048012  	Weight 1: 0.187689  	Weight 2: -2.102132  	Weight 3: -0.045193  
neuron 1, biase -0.748838
	Weight 0: 0.085419  	Weight 1: -1.215402  	Weight 2: -0.986685  	Weight 3: -0.548901  
neuron 2, biase 0.734800
	Weight 0: -0.522619  	Weight 1: 0.823921  	Weight 2: 0.439434  	Weight 3: 1.210563  
neuron 3, biase 0.157078
	Weight 0: 1.364353  	Weight 1: -0.160597  	Weight 2: -0.237843  	Weight 3: 0.185108  
neuron 4, biase -0.014591
	Weight 0: -0.905468  	Weight 1: -0.100968  	Weight 2: -0.324625  	Weight 3: 1.351889  

output layer: 
neuron 0, bias -1.410615
	Weight 0: -0.199343  	Weight 1: 0.537715  	Weight 2: -0.608669  	Weight 3: 0.263507  	Weight 4: 0.321736  
neuron 1, bias -0.729348
	Weight 0: 0.261619  	Weight 1: -1.317629  	Weight 2: 0.132630  	Weight 3: -1.720837  	Weight 4: -0.703797  
neuron 2, bias -0.212769
	Weight 0: -1.152892  	Weight 1: 1.492653  	Weight 2: -0.406960  	Weight 3: 1.602841  	Weight 4: 0.511070  


logit 0: -0.913415 

logit 1: -1.081572 

logit 2: 0.010439 

logit 3: -1.514845 


correct index is 1

prob [0] = 0.229137

prob [1] = 0.193671

prob [2] = 0.577192

correct index is 5

prob [3] = 0.116117

prob [4] = 0.480943

prob [5] = 0.402940

correct index is 6

prob [6] = 0.024517

prob [7] = 0.935926

prob [8] = 0.039557

loss after back = 2.086321

epoch 1 


logit 0: -0.913415 

logit 1: -1.081572 

logit 2: 0.010439 

logit 3: -1.514845 

correct index is 1

prob [0] = 0.229137

prob [1] = 0.193671

prob [2] = 0.577192

correct index is 5

prob [3] = 0.116117

prob [4] = 0.480943

prob [5] = 0.402940

correct index is 6

prob [6] = 0.024517

prob [7] = 0.935926

prob [8] = 0.039557

loss before back = 2.086321

 backwards pass ...

 0.229137  0.116117  -0.975483 

 total delta = -0.063023 

 bias [0] = -1.410615  delta = -0.021008 

 -0.806329  0.480943  0.935926 

 total delta = 0.040046 

 bias [1] = -0.729348  delta = 0.013349 

 0.577192  -0.597060  0.039557 

 total delta = 0.015318 

 bias [2] = -0.212769  delta = 0.005106 

 neuron 0, weight 0 batch 0 grad: 0.051670

 neuron 0, weight 0 batch 1 grad: -0.101749

 neuron 0, weight 0 batch 2 grad: 0.945536

weight delta[0] = 0.895458

 neuron 0, weight 1 batch 0 grad: 0.203325

 neuron 0, weight 1 batch 1 grad: 0.013271

 neuron 0, weight 1 batch 2 grad: 0.869617

weight delta[1] = 1.086212

 neuron 0, weight 2 batch 0 grad: 0.004394

 neuron 0, weight 2 batch 1 grad: 0.071251

 neuron 0, weight 2 batch 2 grad: -0.868298

weight delta[2] = -0.792654

 neuron 0, weight 3 batch 0 grad: -0.184840

 neuron 0, weight 3 batch 1 grad: -0.102131

 neuron 0, weight 3 batch 2 grad: 0.772002

weight delta[3] = 0.485032

 neuron 0, weight 4 batch 0 grad: 0.206000

 neuron 0, weight 4 batch 1 grad: 0.095601

 neuron 0, weight 4 batch 2 grad: -0.453271

weight delta[4] = -0.151669

 neuron 1, weight 0 batch 0 grad: -0.181827

 neuron 1, weight 0 batch 1 grad: -0.421431

 neuron 1, weight 0 batch 2 grad: -0.907194

weight delta[0] = -1.510452

 neuron 1, weight 1 batch 0 grad: -0.715495

 neuron 1, weight 1 batch 1 grad: 0.054968

 neuron 1, weight 1 batch 2 grad: -0.834353

weight delta[1] = -1.494880

 neuron 1, weight 2 batch 0 grad: -0.015461

 neuron 1, weight 2 batch 1 grad: 0.295112

 neuron 1, weight 2 batch 2 grad: 0.833088

weight delta[2] = 1.112739

 neuron 1, weight 3 batch 0 grad: 0.650449

 neuron 1, weight 3 batch 1 grad: -0.423013

 neuron 1, weight 3 batch 2 grad: -0.740697

weight delta[3] = -0.513260

 neuron 1, weight 4 batch 0 grad: -0.724909

 neuron 1, weight 4 batch 1 grad: 0.395970

 neuron 1, weight 4 batch 2 grad: 0.434890

weight delta[4] = 0.105951

 neuron 2, weight 0 batch 0 grad: 0.130157

 neuron 2, weight 0 batch 1 grad: 0.523180

 neuron 2, weight 0 batch 2 grad: -0.038342

weight delta[0] = 0.614994

 neuron 2, weight 1 batch 0 grad: 0.512171

 neuron 2, weight 1 batch 1 grad: -0.068239

 neuron 2, weight 1 batch 2 grad: -0.035264

weight delta[1] = 0.408668

 neuron 2, weight 2 batch 0 grad: 0.011067

 neuron 2, weight 2 batch 1 grad: -0.366363

 neuron 2, weight 2 batch 2 grad: 0.035210

weight delta[2] = -0.320085

 neuron 2, weight 3 batch 0 grad: -0.465609

 neuron 2, weight 3 batch 1 grad: 0.525143

 neuron 2, weight 3 batch 2 grad: -0.031305

weight delta[3] = 0.028229

 neuron 2, weight 4 batch 0 grad: 0.518909

 neuron 2, weight 4 batch 1 grad: -0.491572

 neuron 2, weight 4 batch 2 grad: 0.018381

weight delta[4] = 0.045718

 0.229137  0.116117  -0.975483 

 total delta = -0.063023 

 bias [0] = -1.389608  delta = -0.021008 

 -0.806329  0.480943  0.935926 

 total delta = 0.040046 

 bias [1] = -0.742696  delta = 0.013349 

 0.577192  -0.597060  0.039557 

 total delta = 0.015318 

 bias [2] = -0.217875  delta = 0.005106 

embedding table:
token 0:	embedding: -1.100482 	 -0.908231 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 

hidden layer: 
neuron 0, biase 0.060967
	Weight 0: 1.097490  	Weight 1: 0.232311  	Weight 2: -2.078227  	Weight 3: -0.045193  
neuron 1, biase -0.748838
	Weight 0: 0.134034  	Weight 1: -1.180637  	Weight 2: -0.981004  	Weight 3: -0.548901  
neuron 2, biase 0.734800
	Weight 0: -0.456837  	Weight 1: 0.880953  	Weight 2: 0.465949  	Weight 3: 1.210563  
neuron 3, biase 0.157078
	Weight 0: 1.387684  	Weight 1: -0.140444  	Weight 2: -0.228578  	Weight 3: 0.185108  
neuron 4, biase -0.014591
	Weight 0: -0.884641  	Weight 1: -0.084661  	Weight 2: -0.319529  	Weight 3: 1.351889  

output layer: 
neuron 0, bias -1.368600
	Weight 0: -0.229192  	Weight 1: 0.501508  	Weight 2: -0.582247  	Weight 3: 0.247339  	Weight 4: 0.326791  
neuron 1, bias -0.756045
	Weight 0: 0.311967  	Weight 1: -1.267800  	Weight 2: 0.095539  	Weight 3: -1.703729  	Weight 4: -0.707328  
neuron 2, bias -0.222981
	Weight 0: -1.173392  	Weight 1: 1.479030  	Weight 2: -0.396290  	Weight 3: 1.601900  	Weight 4: 0.509546  


logit 0: -0.854980 

logit 1: -1.034560 

logit 2: -0.199004 

logit 3: -1.274808 


correct index is 1

prob [0] = 0.265771

prob [1] = 0.222083

prob [2] = 0.512146

correct index is 5

prob [3] = 0.143450

prob [4] = 0.372691

prob [5] = 0.483859

correct index is 6

prob [6] = 0.031275

prob [7] = 0.926389

prob [8] = 0.042336

loss after back = 1.898539

epoch 2 


logit 0: -0.854980 

logit 1: -1.034560 

logit 2: -0.199004 

logit 3: -1.274808 

correct index is 1

prob [0] = 0.265771

prob [1] = 0.222083

prob [2] = 0.512146

correct index is 5

prob [3] = 0.143450

prob [4] = 0.372691

prob [5] = 0.483859

correct index is 6

prob [6] = 0.031275

prob [7] = 0.926389

prob [8] = 0.042336

loss before back = 1.898539

 backwards pass ...

 0.265771  0.143450  -0.968726 

 total delta = -0.055950 

 bias [0] = -1.368600  delta = -0.018650 

 -0.777917  0.372691  0.926389 

 total delta = 0.033466 

 bias [1] = -0.756045  delta = 0.011155 

 0.512146  -0.516141  0.042336 

 total delta = 0.014990 

 bias [2] = -0.222981  delta = 0.004997 

 neuron 0, weight 0 batch 0 grad: 0.119672

 neuron 0, weight 0 batch 1 grad: -0.125545

 neuron 0, weight 0 batch 2 grad: 0.946559

weight delta[0] = 0.940686

 neuron 0, weight 1 batch 0 grad: 0.228602

 neuron 0, weight 1 batch 1 grad: 0.025179

 neuron 0, weight 1 batch 2 grad: 0.873167

weight delta[1] = 1.126947

 neuron 0, weight 2 batch 0 grad: -0.040899

 neuron 0, weight 2 batch 1 grad: 0.058952

 neuron 0, weight 2 batch 2 grad: -0.865442

weight delta[2] = -0.847389

 neuron 0, weight 3 batch 0 grad: -0.203284

 neuron 0, weight 3 batch 1 grad: -0.121369

 neuron 0, weight 3 batch 2 grad: 0.820476

weight delta[3] = 0.495823

 neuron 0, weight 4 batch 0 grad: 0.231812

 neuron 0, weight 4 batch 1 grad: 0.111377

 neuron 0, weight 4 batch 2 grad: -0.513414

weight delta[4] = -0.170224

 neuron 1, weight 0 batch 0 grad: -0.350283

 neuron 1, weight 0 batch 1 grad: -0.326174

 neuron 1, weight 0 batch 2 grad: -0.905192

weight delta[0] = -1.581648

 neuron 1, weight 1 batch 0 grad: -0.669122

 neuron 1, weight 1 batch 1 grad: 0.065415

 neuron 1, weight 1 batch 2 grad: -0.835007

weight delta[1] = -1.438714

 neuron 1, weight 2 batch 0 grad: 0.119713

 neuron 1, weight 2 batch 1 grad: 0.153160

 neuron 1, weight 2 batch 2 grad: 0.827619

weight delta[2] = 1.100493

 neuron 1, weight 3 batch 0 grad: 0.595018

 neuron 1, weight 3 batch 1 grad: -0.315323

 neuron 1, weight 3 batch 2 grad: -0.784619

weight delta[3] = -0.504924

 neuron 1, weight 4 batch 0 grad: -0.678520

 neuron 1, weight 4 batch 1 grad: 0.289364

 neuron 1, weight 4 batch 2 grad: 0.490976

weight delta[4] = 0.101821

 neuron 2, weight 0 batch 0 grad: 0.230611

 neuron 2, weight 0 batch 1 grad: 0.451719

 neuron 2, weight 0 batch 2 grad: -0.041368

weight delta[0] = 0.640962

 neuron 2, weight 1 batch 0 grad: 0.440521

 neuron 2, weight 1 batch 1 grad: -0.090594

 neuron 2, weight 1 batch 2 grad: -0.038160

weight delta[1] = 0.311767

 neuron 2, weight 2 batch 0 grad: -0.078814

 neuron 2, weight 2 batch 1 grad: -0.212112

 neuron 2, weight 2 batch 2 grad: 0.037822

weight delta[2] = -0.253104

 neuron 2, weight 3 batch 0 grad: -0.391733

 neuron 2, weight 3 batch 1 grad: 0.436692

 neuron 2, weight 3 batch 2 grad: -0.035857

weight delta[3] = 0.009101

 neuron 2, weight 4 batch 0 grad: 0.446708

 neuron 2, weight 4 batch 1 grad: -0.400742

 neuron 2, weight 4 batch 2 grad: 0.022438

weight delta[4] = 0.068403

 0.265771  0.143450  -0.968726 

 total delta = -0.055950 

 bias [0] = -1.349950  delta = -0.018650 

 -0.777917  0.372691  0.926389 

 total delta = 0.033466 

 bias [1] = -0.767201  delta = 0.011155 

 0.512146  -0.516141  0.042336 

 total delta = 0.014990 

 bias [2] = -0.227977  delta = 0.004997 

embedding table:
token 0:	embedding: -0.884115 	 -1.154866 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 

hidden layer: 
neuron 0, biase 0.060967
	Weight 0: 1.136691  	Weight 1: 0.268644  	Weight 2: -2.055018  	Weight 3: -0.045193  
neuron 1, biase -0.748838
	Weight 0: 0.179130  	Weight 1: -1.141752  	Weight 2: -0.973082  	Weight 3: -0.548901  
neuron 2, biase 0.734800
	Weight 0: -0.390535  	Weight 1: 0.941929  	Weight 2: 0.495546  	Weight 3: 1.210563  
neuron 3, biase 0.157078
	Weight 0: 1.413329  	Weight 1: -0.116620  	Weight 2: -0.216002  	Weight 3: 0.185108  
neuron 4, biase -0.014591
	Weight 0: -0.861296  	Weight 1: -0.063862  	Weight 2: -0.312268  	Weight 3: 1.351889  

output layer: 
neuron 0, bias -1.331300
	Weight 0: -0.260548  	Weight 1: 0.463943  	Weight 2: -0.554001  	Weight 3: 0.230812  	Weight 4: 0.332465  
neuron 1, bias -0.778356
	Weight 0: 0.364689  	Weight 1: -1.219843  	Weight 2: 0.058856  	Weight 3: -1.686898  	Weight 4: -0.710722  
neuron 2, bias -0.232974
	Weight 0: -1.194757  	Weight 1: 1.468638  	Weight 2: -0.387854  	Weight 3: 1.601596  	Weight 4: 0.507266  


logit 0: -0.855321 

logit 1: -1.043373 

logit 2: -0.458845 

logit 3: -0.871093 


correct index is 1

prob [0] = 0.301646

prob [1] = 0.249935

prob [2] = 0.448419

correct index is 5

prob [3] = 0.164889

prob [4] = 0.178519

prob [5] = 0.656592

correct index is 6

prob [6] = 0.040527

prob [7] = 0.914656

prob [8] = 0.044817

loss after back = 1.671014

epoch 3 


logit 0: -0.855321 

logit 1: -1.043373 

logit 2: -0.458845 

logit 3: -0.871093 

correct index is 1

prob [0] = 0.301646

prob [1] = 0.249935

prob [2] = 0.448419

correct index is 5

prob [3] = 0.164889

prob [4] = 0.178519

prob [5] = 0.656592

correct index is 6

prob [6] = 0.040527

prob [7] = 0.914656

prob [8] = 0.044817

loss before back = 1.671014

 backwards pass ...

 0.301646  0.164889  -0.959473 

 total delta = -0.049294 

 bias [0] = -1.331300  delta = -0.016431 

 -0.750065  0.178519  0.914656 

 total delta = 0.017880 

 bias [1] = -0.778356  delta = 0.005960 

 0.448419  -0.343408  0.044817 

 total delta = 0.020943 

 bias [2] = -0.232974  delta = 0.006981 

 neuron 0, weight 0 batch 0 grad: 0.250705

 neuron 0, weight 0 batch 1 grad: -0.139932

 neuron 0, weight 0 batch 2 grad: 0.947399

weight delta[0] = 1.058172

 neuron 0, weight 1 batch 0 grad: 0.253810

 neuron 0, weight 1 batch 1 grad: 0.064440

 neuron 0, weight 1 batch 2 grad: 0.878270

weight delta[1] = 1.196520

 neuron 0, weight 2 batch 0 grad: -0.094719

 neuron 0, weight 2 batch 1 grad: -0.001391

 neuron 0, weight 2 batch 2 grad: -0.867221

weight delta[2] = -0.963331

 neuron 0, weight 3 batch 0 grad: -0.189762

 neuron 0, weight 3 batch 1 grad: -0.122537

 neuron 0, weight 3 batch 2 grad: 0.885661

weight delta[3] = 0.573362

 neuron 0, weight 4 batch 0 grad: 0.248053

 neuron 0, weight 4 batch 1 grad: 0.111409

 neuron 0, weight 4 batch 2 grad: -0.629460

weight delta[4] = -0.269997

 neuron 1, weight 0 batch 0 grad: -0.623397

 neuron 1, weight 0 batch 1 grad: -0.151500

 neuron 1, weight 0 batch 2 grad: -0.903146

weight delta[0] = -1.678043

 neuron 1, weight 1 batch 0 grad: -0.631118

 neuron 1, weight 1 batch 1 grad: 0.069767

 neuron 1, weight 1 batch 2 grad: -0.837246

weight delta[1] = -1.398597

 neuron 1, weight 2 batch 0 grad: 0.235526

 neuron 1, weight 2 batch 1 grad: -0.001506

 neuron 1, weight 2 batch 2 grad: 0.826713

weight delta[2] = 1.060733

 neuron 1, weight 3 batch 0 grad: 0.471859

 neuron 1, weight 3 batch 1 grad: -0.132667

 neuron 1, weight 3 batch 2 grad: -0.844291

weight delta[3] = -0.505100

 neuron 1, weight 4 batch 0 grad: -0.616803

 neuron 1, weight 4 batch 1 grad: 0.120619

 neuron 1, weight 4 batch 2 grad: 0.600058

weight delta[4] = 0.103873

 neuron 2, weight 0 batch 0 grad: 0.372692

 neuron 2, weight 0 batch 1 grad: 0.291432

 neuron 2, weight 0 batch 2 grad: -0.044253

weight delta[0] = 0.619871

 neuron 2, weight 1 batch 0 grad: 0.377308

 neuron 2, weight 1 batch 1 grad: -0.134208

 neuron 2, weight 1 batch 2 grad: -0.041024

weight delta[1] = 0.202076

 neuron 2, weight 2 batch 0 grad: -0.140807

 neuron 2, weight 2 batch 1 grad: 0.002896

 neuron 2, weight 2 batch 2 grad: 0.040508

weight delta[2] = -0.097402

 neuron 2, weight 3 batch 0 grad: -0.282096

 neuron 2, weight 3 batch 1 grad: 0.255204

 neuron 2, weight 3 batch 2 grad: -0.041370

weight delta[3] = -0.068262

 neuron 2, weight 4 batch 0 grad: 0.368750

 neuron 2, weight 4 batch 1 grad: -0.232029

 neuron 2, weight 4 batch 2 grad: 0.029402

weight delta[4] = 0.166124

 0.301646  0.164889  -0.959473 

 total delta = -0.049294 

 bias [0] = -1.314868  delta = -0.016431 

 -0.750065  0.178519  0.914656 

 total delta = 0.017880 

 bias [1] = -0.784316  delta = 0.005960 

 0.448419  -0.343408  0.044817 

 total delta = 0.020943 

 bias [2] = -0.239955  delta = 0.006981 

embedding table:
token 0:	embedding: -0.572288 	 -1.599323 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 

hidden layer: 
neuron 0, biase 0.060967
	Weight 0: 1.155012  	Weight 1: 0.288529  	Weight 2: -2.043620  	Weight 3: -0.045193  
neuron 1, biase -0.748838
	Weight 0: 0.212705  	Weight 1: -1.100530  	Weight 2: -0.961800  	Weight 3: -0.548901  
neuron 2, biase 0.734800
	Weight 0: -0.334502  	Weight 1: 1.006986  	Weight 2: 0.530294  	Weight 3: 1.210563  
neuron 3, biase 0.157078
	Weight 0: 1.444331  	Weight 1: -0.081577  	Weight 2: -0.192720  	Weight 3: 0.185108  
neuron 4, biase -0.014591
	Weight 0: -0.835738  	Weight 1: -0.033399  	Weight 2: -0.299778  	Weight 3: 1.351889  

output layer: 
neuron 0, bias -1.298437
	Weight 0: -0.295820  	Weight 1: 0.424059  	Weight 2: -0.521890  	Weight 3: 0.211700  	Weight 4: 0.341465  
neuron 1, bias -0.790276
	Weight 0: 0.420624  	Weight 1: -1.173223  	Weight 2: 0.023498  	Weight 3: -1.670061  	Weight 4: -0.714185  
neuron 2, bias -0.246936
	Weight 0: -1.215420  	Weight 1: 1.461902  	Weight 2: -0.384607  	Weight 3: 1.603872  	Weight 4: 0.501728  


logit 0: -0.794755 

logit 1: -1.419857 

logit 2: -0.124830 

logit 3: -0.395611 


correct index is 1

prob [0] = 0.286591

prob [1] = 0.153385

prob [2] = 0.560024

correct index is 5

prob [3] = 0.132998

prob [4] = 0.044623

prob [5] = 0.822379

correct index is 6

prob [6] = 0.055864

prob [7] = 0.893994

prob [8] = 0.050142

loss after back = 1.651731

epoch 4 


logit 0: -0.794755 

logit 1: -1.419857 

logit 2: -0.124830 

logit 3: -0.395611 

correct index is 1

prob [0] = 0.286591

prob [1] = 0.153385

prob [2] = 0.560024

correct index is 5

prob [3] = 0.132998

prob [4] = 0.044623

prob [5] = 0.822379

correct index is 6

prob [6] = 0.055864

prob [7] = 0.893994

prob [8] = 0.050142

loss before back = 1.651731

 backwards pass ...

 0.286591  0.132998  -0.944136 

 total delta = -0.052455 

 bias [0] = -1.298437  delta = -0.017485 

 -0.846615  0.044623  0.893994 

 total delta = -0.008285 

 bias [1] = -0.790276  delta = -0.002762 

 0.560024  -0.177621  0.050142 

 total delta = 0.040493 

 bias [2] = -0.246936  delta = 0.013498 

 neuron 0, weight 0 batch 0 grad: 0.282779

 neuron 0, weight 0 batch 1 grad: -0.104418

 neuron 0, weight 0 batch 2 grad: 0.939982

weight delta[0] = 1.118342

 neuron 0, weight 1 batch 0 grad: 0.246776

 neuron 0, weight 1 batch 1 grad: 0.094674

 neuron 0, weight 1 batch 2 grad: 0.881482

weight delta[1] = 1.222932

 neuron 0, weight 2 batch 0 grad: -0.132025

 neuron 0, weight 2 batch 1 grad: -0.079104

 neuron 0, weight 2 batch 2 grad: -0.869947

weight delta[2] = -1.081076

 neuron 0, weight 3 batch 0 grad: -0.087297

 neuron 0, weight 3 batch 1 grad: -0.065441

 neuron 0, weight 3 batch 2 grad: 0.924301

weight delta[3] = 0.771562

 neuron 0, weight 4 batch 0 grad: 0.213590

 neuron 0, weight 4 batch 1 grad: 0.063280

 neuron 0, weight 4 batch 2 grad: -0.767483

weight delta[4] = -0.490614

 neuron 1, weight 0 batch 0 grad: -0.835355

 neuron 1, weight 0 batch 1 grad: -0.035034

 neuron 1, weight 0 batch 2 grad: -0.890061

weight delta[0] = -1.760450

 neuron 1, weight 1 batch 0 grad: -0.728998

 neuron 1, weight 1 batch 1 grad: 0.031764

 neuron 1, weight 1 batch 2 grad: -0.834668

weight delta[1] = -1.531901

 neuron 1, weight 2 batch 0 grad: 0.390013

 neuron 1, weight 2 batch 1 grad: -0.026540

 neuron 1, weight 2 batch 2 grad: 0.823746

weight delta[2] = 1.187218

 neuron 1, weight 3 batch 0 grad: 0.257884

 neuron 1, weight 3 batch 1 grad: -0.021956

 neuron 1, weight 3 batch 2 grad: -0.875212

weight delta[3] = -0.639284

 neuron 1, weight 4 batch 0 grad: -0.630963

 neuron 1, weight 4 batch 1 grad: 0.021231

 neuron 1, weight 4 batch 2 grad: 0.726723

weight delta[4] = 0.116992

 neuron 2, weight 0 batch 0 grad: 0.552576

 neuron 2, weight 0 batch 1 grad: 0.139452

 neuron 2, weight 0 batch 2 grad: -0.049921

weight delta[0] = 0.642107

 neuron 2, weight 1 batch 0 grad: 0.482222

 neuron 2, weight 1 batch 1 grad: -0.126439

 neuron 2, weight 1 batch 2 grad: -0.046814

weight delta[1] = 0.308969

 neuron 2, weight 2 batch 0 grad: -0.257988

 neuron 2, weight 2 batch 1 grad: 0.105645

 neuron 2, weight 2 batch 2 grad: 0.046202

weight delta[2] = -0.106142

 neuron 2, weight 3 batch 0 grad: -0.170587

 neuron 2, weight 3 batch 1 grad: 0.087397

 neuron 2, weight 3 batch 2 grad: -0.049088

weight delta[3] = -0.132278

 neuron 2, weight 4 batch 0 grad: 0.417373

 neuron 2, weight 4 batch 1 grad: -0.084512

 neuron 2, weight 4 batch 2 grad: 0.040760

weight delta[4] = 0.373622

 0.286591  0.132998  -0.944136 

 total delta = -0.052455 

 bias [0] = -1.280952  delta = -0.017485 

 -0.846615  0.044623  0.893994 

 total delta = -0.008285 

 bias [1] = -0.787514  delta = -0.002762 

 0.560024  -0.177621  0.050142 

 total delta = 0.040493 

 bias [2] = -0.260433  delta = 0.013498 

embedding table:
token 0:	embedding: -0.209924 	 -2.312864 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 

hidden layer: 
neuron 0, biase 0.060967
	Weight 0: 1.163302  	Weight 1: 0.309484  	Weight 2: -2.042375  	Weight 3: -0.045193  
neuron 1, biase -0.748838
	Weight 0: 0.227047  	Weight 1: -1.069301  	Weight 2: -0.947993  	Weight 3: -0.548901  
neuron 2, biase 0.734800
	Weight 0: -0.307146  	Weight 1: 1.056466  	Weight 2: 0.572322  	Weight 3: 1.210563  
neuron 3, biase 0.157078
	Weight 0: 1.476095  	Weight 1: -0.023866  	Weight 2: -0.144319  	Weight 3: 0.185108  
neuron 4, biase -0.014591
	Weight 0: -0.812500  	Weight 1: 0.016324  	Weight 2: -0.276041  	Weight 3: 1.351889  

output layer: 
neuron 0, bias -1.263467
	Weight 0: -0.333098  	Weight 1: 0.383294  	Weight 2: -0.485854  	Weight 3: 0.185981  	Weight 4: 0.357819  
neuron 1, bias -0.784753
	Weight 0: 0.479305  	Weight 1: -1.122159  	Weight 2: -0.016076  	Weight 3: -1.648752  	Weight 4: -0.718085  
neuron 2, bias -0.273931
	Weight 0: -1.236823  	Weight 1: 1.451603  	Weight 2: -0.381069  	Weight 3: 1.608281  	Weight 4: 0.489274  


logit 0: -0.664567 

logit 1: -2.109456 

logit 2: 0.691211 

logit 3: -0.192898 


correct index is 1

prob [0] = 0.195482

prob [1] = 0.046089

prob [2] = 0.758428

correct index is 5

prob [3] = 0.081279

prob [4] = 0.012272

prob [5] = 0.906450

correct index is 6

prob [6] = 0.080429

prob [7] = 0.861296

prob [8] = 0.058275

 new loss is greater: 1.898592
