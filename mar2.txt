
1 names loaded

creating model ...

... model created.

initialising model ....

model initialised ....

model before training:

embedding table:
token 0:	embedding: -1.255212 	 -0.760357 
token 1:	embedding: -0.001434 	 0.622350 
token 2:	embedding: 1.529260 	 0.136996 
token 3:	embedding: 0.996038 	 0.141729 

hidden layer: 
neuron 0, biase 0.734800
	Weight 0: -2.124576  	Weight 1: -0.045193  	Weight 2: 0.037315  	Weight 3: -1.247397  
neuron 1, biase 0.157078
	Weight 0: -0.991119  	Weight 1: -0.548901  	Weight 2: -0.583988  	Weight 3: 0.770801  
neuron 2, biase -0.014591
	Weight 0: 0.414911  	Weight 1: 1.210563  	Weight 2: 1.340327  	Weight 3: -0.180978  
neuron 3, biase -0.168911
	Weight 0: -0.246805  	Weight 1: 0.185108  	Weight 2: -0.924544  	Weight 3: -0.115292  
neuron 4, biase 0.573566
	Weight 0: -0.328892  	Weight 1: 1.351889  	Weight 2: 0.060967  	Weight 3: -0.748838  

output layer: 
neuron 0, bias -0.560132
	Weight 0: -0.634222  	Weight 1: 0.279987  	Weight 2: 0.316312  	Weight 3: 0.210182  	Weight 4: -1.367247  
neuron 1, bias 1.401056
	Weight 0: 0.167800  	Weight 1: -1.738395  	Weight 2: -0.699830  	Weight 3: -1.131888  	Weight 4: 1.506419  
neuron 2, bias 1.285370
	Weight 0: -0.416576  	Weight 1: 1.603918  	Weight 2: 0.512526  	Weight 3: -1.455196  	Weight 4: -0.700985  
neuron 3, bias -0.785712
	Weight 0: -0.201957  	Weight 1: 0.563913  	Weight 2: -0.154251  	Weight 3: 0.167294  	Weight 4: 0.158440  

epoch 0 


logit 0: -0.242928 

logit 1: -1.031348 

logit 2: 1.597911 

logit 3: -0.286864 

correct index is 1

prob [0] = 0.114766

prob [1] = 0.052168

prob [2] = 0.723234

correct index is 6

prob [4] = 0.032341

prob [5] = 0.166189

prob [6] = 0.727015

correct index is 8

prob [8] = 0.008501

prob [9] = 0.441340

prob [10] = 0.521757

loss before back = 2.679895

 backwards pass ...
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 8
offset_batch_embedding_element: 9
offset_batch_embedding_element: 10
offset_batch_embedding_element: 11
offset_batch_embedding_element: 18
offset_batch_embedding_element: 19
offset_batch_embedding_element: 20
offset_batch_embedding_element: 21

 0.114766  0.032341  -0.991499 

 total delta = -0.084439 

 bias [0] = -0.560132  delta = -0.028146 

 -0.947832  0.166189  0.441340 

 total delta = -0.062177 

 bias [1] = 1.401056  delta = -0.020726 

 0.723234  -0.272985  0.521757 

 total delta = 0.076475 

 bias [2] = 1.285370  delta = 0.025492 

 0.109833  0.074454  0.028402 

 total delta = 0.046761 

 bias [3] = -0.785712  delta = 0.015587 

 neuron 0, weight 0 batch 0 grad: 0.114525

 neuron 0, weight 0 batch 1 grad: 0.032274

 neuron 0, weight 0 batch 2 grad: -0.974440

weight delta[0] = -0.827641

 neuron 0, weight 1 batch 0 grad: 0.113330

 neuron 0, weight 1 batch 1 grad: 0.030684

 neuron 0, weight 1 batch 2 grad: -0.494795

weight delta[1] = -0.350781

 neuron 0, weight 2 batch 0 grad: -0.114276

 neuron 0, weight 2 batch 1 grad: -0.029017

 neuron 0, weight 2 batch 2 grad: -0.460015

weight delta[2] = -0.603308

 neuron 0, weight 3 batch 0 grad: 0.062570

 neuron 0, weight 3 batch 1 grad: 0.000047

 neuron 0, weight 3 batch 2 grad: 0.501380

weight delta[3] = 0.563996

 neuron 0, weight 4 batch 0 grad: -0.073362

 neuron 0, weight 4 batch 1 grad: -0.001345

 neuron 0, weight 4 batch 2 grad: -0.690158

weight delta[4] = -0.764864

 neuron 1, weight 0 batch 0 grad: -0.945845

 neuron 1, weight 0 batch 1 grad: 0.165845

 neuron 1, weight 0 batch 2 grad: 0.433747

weight delta[0] = -0.346253

 neuron 1, weight 1 batch 0 grad: -0.935977

 neuron 1, weight 1 batch 1 grad: 0.157676

 neuron 1, weight 1 batch 2 grad: 0.220245

weight delta[1] = -0.558056

 neuron 1, weight 2 batch 0 grad: 0.943788

 neuron 1, weight 2 batch 1 grad: -0.149108

 neuron 1, weight 2 batch 2 grad: 0.204764

weight delta[2] = 0.999444

 neuron 1, weight 3 batch 0 grad: -0.516751

 neuron 1, weight 3 batch 1 grad: 0.000243

 neuron 1, weight 3 batch 2 grad: -0.223176

weight delta[3] = -0.739685

 neuron 1, weight 4 batch 0 grad: 0.605881

 neuron 1, weight 4 batch 1 grad: -0.006911

 neuron 1, weight 4 batch 2 grad: 0.307206

weight delta[4] = 0.906176

 neuron 2, weight 0 batch 0 grad: 0.721718

 neuron 2, weight 0 batch 1 grad: -0.272419

 neuron 2, weight 0 batch 2 grad: 0.512780

weight delta[0] = 0.962079

 neuron 2, weight 1 batch 0 grad: 0.714188

 neuron 2, weight 1 batch 1 grad: -0.259000

 neuron 2, weight 1 batch 2 grad: 0.260376

weight delta[1] = 0.715564

 neuron 2, weight 2 batch 0 grad: -0.720148

 neuron 2, weight 2 batch 1 grad: 0.244927

 neuron 2, weight 2 batch 2 grad: 0.242074

weight delta[2] = -0.233147

 neuron 2, weight 3 batch 0 grad: 0.394302

 neuron 2, weight 3 batch 1 grad: -0.000398

 neuron 2, weight 3 batch 2 grad: -0.263841

weight delta[3] = 0.130062

 neuron 2, weight 4 batch 0 grad: -0.462312

 neuron 2, weight 4 batch 1 grad: 0.011352

 neuron 2, weight 4 batch 2 grad: 0.363182

weight delta[4] = -0.087777

 neuron 3, weight 0 batch 0 grad: 0.109602

 neuron 3, weight 0 batch 1 grad: 0.074300

 neuron 3, weight 0 batch 2 grad: 0.027913

weight delta[0] = 0.211816

 neuron 3, weight 1 batch 0 grad: 0.108459

 neuron 3, weight 1 batch 1 grad: 0.070640

 neuron 3, weight 1 batch 2 grad: 0.014174

weight delta[1] = 0.193273

 neuron 3, weight 2 batch 0 grad: -0.109364

 neuron 3, weight 2 batch 1 grad: -0.066802

 neuron 3, weight 2 batch 2 grad: 0.013177

weight delta[2] = -0.162989

 neuron 3, weight 3 batch 0 grad: 0.059880

 neuron 3, weight 3 batch 1 grad: 0.000109

 neuron 3, weight 3 batch 2 grad: -0.014362

weight delta[3] = 0.045626

 neuron 3, weight 4 batch 0 grad: -0.070208

 neuron 3, weight 4 batch 1 grad: -0.003096

 neuron 3, weight 4 batch 2 grad: 0.019770

weight delta[4] = -0.053535

 0.114766  0.032341  -0.991499 

 total delta = -0.084439 

 bias [0] = -0.531985  delta = -0.028146 

 -0.947832  0.166189  0.441340 

 total delta = -0.062177 

 bias [1] = 1.421781  delta = -0.020726 

 0.723234  -0.272985  0.521757 

 total delta = 0.076475 

 bias [2] = 1.259878  delta = 0.025492 

 0.109833  0.074454  0.028402 

 total delta = 0.046761 

 bias [3] = -0.801299  delta = 0.015587 

embedding table:
token 0:	embedding: -1.252261 	 -0.753622 
token 1:	embedding: 0.015615 	 0.642415 
token 2:	embedding: 1.549814 	 0.154045 
token 3:	embedding: 1.013087 	 0.161795 

hidden layer: 
neuron 0, biase 0.734800
	Weight 0: -2.123363  	Weight 1: -0.044911  	Weight 2: 0.036714  	Weight 3: -1.247397  
neuron 1, biase 0.157078
	Weight 0: -0.985902  	Weight 1: -0.545331  	Weight 2: -0.583354  	Weight 3: 0.770801  
neuron 2, biase -0.014591
	Weight 0: 0.423426  	Weight 1: 1.215861  	Weight 2: 1.340552  	Weight 3: -0.180978  
neuron 3, biase -0.168911
	Weight 0: -0.175561  	Weight 1: 0.239857  	Weight 2: -0.906684  	Weight 3: -0.115292  
neuron 4, biase 0.573566
	Weight 0: -0.262380  	Weight 1: 1.401934  	Weight 2: 0.076003  	Weight 3: -0.748838  

output layer: 
neuron 0, bias -0.503839
	Weight 0: -0.606634  	Weight 1: 0.291680  	Weight 2: 0.336422  	Weight 3: 0.191382  	Weight 4: -1.341752  
neuron 1, bias 1.442507
	Weight 0: 0.179342  	Weight 1: -1.719793  	Weight 2: -0.733144  	Weight 3: -1.107232  	Weight 4: 1.476213  
neuron 2, bias 1.234387
	Weight 0: -0.448646  	Weight 1: 1.580066  	Weight 2: 0.520298  	Weight 3: -1.459532  	Weight 4: -0.698059  
neuron 3, bias -0.816886
	Weight 0: -0.209018  	Weight 1: 0.557471  	Weight 2: -0.148818  	Weight 3: 0.165773  	Weight 4: 0.160225  


logit 0: -0.110055 

logit 1: -0.863330 

logit 2: 1.735450 

logit 3: -0.374927 


correct index is 1

prob [0] = 0.116694

prob [1] = 0.054942

prob [2] = 0.738824

correct index is 6

prob [4] = 0.034082

prob [5] = 0.159101

prob [6] = 0.748086

correct index is 8

prob [8] = 0.009649

prob [9] = 0.465854

prob [10] = 0.498059

loss after back = 2.610868

epoch 1 


logit 0: -0.110055 

logit 1: -0.863330 

logit 2: 1.735450 

logit 3: -0.374927 

correct index is 1

prob [0] = 0.116694

prob [1] = 0.054942

prob [2] = 0.738824

correct index is 6

prob [4] = 0.034082

prob [5] = 0.159101

prob [6] = 0.748086

correct index is 8

prob [8] = 0.009649

prob [9] = 0.465854

prob [10] = 0.498059

loss before back = 2.610868

 backwards pass ...
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 8
offset_batch_embedding_element: 9
offset_batch_embedding_element: 10
offset_batch_embedding_element: 11
offset_batch_embedding_element: 18
offset_batch_embedding_element: 19
offset_batch_embedding_element: 20
offset_batch_embedding_element: 21

 0.116694  0.034082  -0.990351 

 total delta = -0.083957 

 bias [0] = -0.503839  delta = -0.027986 

 -0.945058  0.159101  0.465854 

 total delta = -0.059996 

 bias [1] = 1.442507  delta = -0.019999 

 0.738824  -0.251914  0.498059 

 total delta = 0.078498 

 bias [2] = 1.234387  delta = 0.026166 

 0.089540  0.058732  0.026438 

 total delta = 0.043637 

 bias [3] = -0.816886  delta = 0.014546 

 neuron 0, weight 0 batch 0 grad: 0.116446

 neuron 0, weight 0 batch 1 grad: 0.034010

 neuron 0, weight 0 batch 2 grad: -0.972776

weight delta[0] = -0.822320

 neuron 0, weight 1 batch 0 grad: 0.115175

 neuron 0, weight 1 batch 1 grad: 0.032246

 neuron 0, weight 1 batch 2 grad: -0.470644

weight delta[1] = -0.323222

 neuron 0, weight 2 batch 0 grad: -0.116200

 neuron 0, weight 2 batch 1 grad: -0.030460

 neuron 0, weight 2 batch 2 grad: -0.493075

weight delta[2] = -0.639735

 neuron 0, weight 3 batch 0 grad: 0.047675

 neuron 0, weight 3 batch 1 grad: -0.004874

 neuron 0, weight 3 batch 2 grad: 0.542579

weight delta[3] = 0.585381

 neuron 0, weight 4 batch 0 grad: -0.084193

 neuron 0, weight 4 batch 1 grad: -0.005181

 neuron 0, weight 4 batch 2 grad: -0.680249

weight delta[4] = -0.769623

 neuron 1, weight 0 batch 0 grad: -0.943047

 neuron 1, weight 0 batch 1 grad: 0.158766

 neuron 1, weight 0 batch 2 grad: 0.457587

weight delta[0] = -0.326694

 neuron 1, weight 1 batch 0 grad: -0.932760

 neuron 1, weight 1 batch 1 grad: 0.150530

 neuron 1, weight 1 batch 2 grad: 0.221388

weight delta[1] = -0.560842

 neuron 1, weight 2 batch 0 grad: 0.941055

 neuron 1, weight 2 batch 1 grad: -0.142194

 neuron 1, weight 2 batch 2 grad: 0.231939

weight delta[2] = 1.030800

 neuron 1, weight 3 batch 0 grad: -0.386104

 neuron 1, weight 3 batch 1 grad: -0.022751

 neuron 1, weight 3 batch 2 grad: -0.255226

weight delta[3] = -0.664080

 neuron 1, weight 4 batch 0 grad: 0.681843

 neuron 1, weight 4 batch 1 grad: -0.024186

 neuron 1, weight 4 batch 2 grad: 0.319984

weight delta[4] = 0.977641

 neuron 2, weight 0 batch 0 grad: 0.737252

 neuron 2, weight 0 batch 1 grad: -0.251385

 neuron 2, weight 0 batch 2 grad: 0.489220

weight delta[0] = 0.975087

 neuron 2, weight 1 batch 0 grad: 0.729210

 neuron 2, weight 1 batch 1 grad: -0.238345

 neuron 2, weight 1 batch 2 grad: 0.236692

weight delta[1] = 0.727557

 neuron 2, weight 2 batch 0 grad: -0.735695

 neuron 2, weight 2 batch 1 grad: 0.225145

 neuron 2, weight 2 batch 2 grad: 0.247973

weight delta[2] = -0.262576

 neuron 2, weight 3 batch 0 grad: 0.301847

 neuron 2, weight 3 batch 1 grad: 0.036023

 neuron 2, weight 3 batch 2 grad: -0.272869

weight delta[3] = 0.065000

 neuron 2, weight 4 batch 0 grad: -0.533049

 neuron 2, weight 4 batch 1 grad: 0.038296

 neuron 2, weight 4 batch 2 grad: 0.342105

weight delta[4] = -0.152648

 neuron 3, weight 0 batch 0 grad: 0.089349

 neuron 3, weight 0 batch 1 grad: 0.058608

 neuron 3, weight 0 batch 2 grad: 0.025969

weight delta[0] = 0.173927

 neuron 3, weight 1 batch 0 grad: 0.088375

 neuron 3, weight 1 batch 1 grad: 0.055568

 neuron 3, weight 1 batch 2 grad: 0.012564

weight delta[1] = 0.156507

 neuron 3, weight 2 batch 0 grad: -0.089161

 neuron 3, weight 2 batch 1 grad: -0.052491

 neuron 3, weight 2 batch 2 grad: 0.013163

weight delta[2] = -0.128489

 neuron 3, weight 3 batch 0 grad: 0.036582

 neuron 3, weight 3 batch 1 grad: -0.008398

 neuron 3, weight 3 batch 2 grad: -0.014484

weight delta[3] = 0.013699

 neuron 3, weight 4 batch 0 grad: -0.064601

 neuron 3, weight 4 batch 1 grad: -0.008928

 neuron 3, weight 4 batch 2 grad: 0.018160

weight delta[4] = -0.055370

 0.116694  0.034082  -0.990351 

 total delta = -0.083957 

 bias [0] = -0.475853  delta = -0.027986 

 -0.945058  0.159101  0.465854 

 total delta = -0.059996 

 bias [1] = 1.462505  delta = -0.019999 

 0.738824  -0.251914  0.498059 

 total delta = 0.078498 

 bias [2] = 1.208221  delta = 0.026166 

 0.089540  0.058732  0.026438 

 total delta = 0.043637 

 bias [3] = -0.831432  delta = 0.014546 

embedding table:
token 0:	embedding: -1.241108 	 -0.730267 
token 1:	embedding: 0.064447 	 0.697724 
token 2:	embedding: 1.615006 	 0.202877 
token 3:	embedding: 1.061919 	 0.217104 

hidden layer: 
neuron 0, biase 0.734800
	Weight 0: -2.122126  	Weight 1: -0.044647  	Weight 2: 0.036065  	Weight 3: -1.247397  
neuron 1, biase 0.157078
	Weight 0: -0.980447  	Weight 1: -0.541618  	Weight 2: -0.582759  	Weight 3: 0.770801  
neuron 2, biase -0.014591
	Weight 0: 0.432179  	Weight 1: 1.221269  	Weight 2: 1.340659  	Weight 3: -0.180978  
neuron 3, biase -0.168911
	Weight 0: -0.099898  	Weight 1: 0.299239  	Weight 2: -0.886266  	Weight 3: -0.115292  
neuron 4, biase 0.573566
	Weight 0: -0.201589  	Weight 1: 1.446488  	Weight 2: 0.087539  	Weight 3: -0.748838  

output layer: 
neuron 0, bias -0.447867
	Weight 0: -0.579223  	Weight 1: 0.302454  	Weight 2: 0.357747  	Weight 3: 0.171870  	Weight 4: -1.316098  
neuron 1, bias 1.482504
	Weight 0: 0.190231  	Weight 1: -1.701098  	Weight 2: -0.767504  	Weight 3: -1.085096  	Weight 4: 1.443625  
neuron 2, bias 1.182055
	Weight 0: -0.481149  	Weight 1: 1.555814  	Weight 2: 0.529050  	Weight 3: -1.461698  	Weight 4: -0.692971  
neuron 3, bias -0.845977
	Weight 0: -0.214815  	Weight 1: 0.552254  	Weight 2: -0.144535  	Weight 3: 0.165317  	Weight 4: 0.162070  


logit 0: -0.023377 

logit 1: -0.607202 

logit 2: 1.915269 

logit 3: -0.460267 


correct index is 1

prob [0] = 0.109252

prob [1] = 0.060937

prob [2] = 0.759230

correct index is 6

prob [4] = 0.033443

prob [5] = 0.161626

prob [6] = 0.759236

correct index is 8

prob [8] = 0.010470

prob [9] = 0.517205

prob [10] = 0.448866

loss after back = 2.544214

epoch 2 


logit 0: -0.023377 

logit 1: -0.607202 

logit 2: 1.915269 

logit 3: -0.460267 

correct index is 1

prob [0] = 0.109252

prob [1] = 0.060937

prob [2] = 0.759230

correct index is 6

prob [4] = 0.033443

prob [5] = 0.161626

prob [6] = 0.759236

correct index is 8

prob [8] = 0.010470

prob [9] = 0.517205

prob [10] = 0.448866

loss before back = 2.544214

 backwards pass ...
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 8
offset_batch_embedding_element: 9
offset_batch_embedding_element: 10
offset_batch_embedding_element: 11
offset_batch_embedding_element: 18
offset_batch_embedding_element: 19
offset_batch_embedding_element: 20
offset_batch_embedding_element: 21

 0.109252  0.033443  -0.989530 

 total delta = -0.084684 

 bias [0] = -0.447867  delta = -0.028228 

 -0.939063  0.161626  0.517205 

 total delta = -0.054251 

 bias [1] = 1.482504  delta = -0.018084 

 0.759230  -0.240764  0.448866 

 total delta = 0.078649 

 bias [2] = 1.182055  delta = 0.026216 

 0.070582  0.045695  0.023460 

 total delta = 0.040190 

 bias [3] = -0.845977  delta = 0.013397 

 neuron 0, weight 0 batch 0 grad: 0.109008

 neuron 0, weight 0 batch 1 grad: 0.033369

 neuron 0, weight 0 batch 2 grad: -0.970099

weight delta[0] = -0.827721

 neuron 0, weight 1 batch 0 grad: 0.107705

 neuron 0, weight 1 batch 1 grad: 0.031412

 neuron 0, weight 1 batch 2 grad: -0.402363

weight delta[1] = -0.263245

 neuron 0, weight 2 batch 0 grad: -0.108759

 neuron 0, weight 2 batch 1 grad: -0.029282

 neuron 0, weight 2 batch 2 grad: -0.587842

weight delta[2] = -0.725883

 neuron 0, weight 3 batch 0 grad: 0.024787

 neuron 0, weight 3 batch 1 grad: -0.010368

 neuron 0, weight 3 batch 2 grad: 0.594918

weight delta[3] = 0.609337

 neuron 0, weight 4 batch 0 grad: -0.084787

 neuron 0, weight 4 batch 1 grad: -0.007461

 neuron 0, weight 4 batch 2 grad: -0.696576

weight delta[4] = -0.788825

 neuron 1, weight 0 batch 0 grad: -0.936964

 neuron 1, weight 0 batch 1 grad: 0.161269

 neuron 1, weight 0 batch 2 grad: 0.507048

weight delta[0] = -0.268647

 neuron 1, weight 1 batch 0 grad: -0.925766

 neuron 1, weight 1 batch 1 grad: 0.151812

 neuron 1, weight 1 batch 2 grad: 0.210306

weight delta[1] = -0.563649

 neuron 1, weight 2 batch 0 grad: 0.934825

 neuron 1, weight 2 batch 1 grad: -0.141513

 neuron 1, weight 2 batch 2 grad: 0.307252

weight delta[2] = 1.100563

 neuron 1, weight 3 batch 0 grad: -0.213054

 neuron 1, weight 3 batch 1 grad: -0.050107

 neuron 1, weight 3 batch 2 grad: -0.310950

weight delta[3] = -0.574111

 neuron 1, weight 4 batch 0 grad: 0.728780

 neuron 1, weight 4 batch 1 grad: -0.036060

 neuron 1, weight 4 batch 2 grad: 0.364084

weight delta[4] = 1.056805

 neuron 2, weight 0 batch 0 grad: 0.757532

 neuron 2, weight 0 batch 1 grad: -0.240232

 neuron 2, weight 0 batch 2 grad: 0.440051

weight delta[0] = 0.957351

 neuron 2, weight 1 batch 0 grad: 0.748479

 neuron 2, weight 1 batch 1 grad: -0.226145

 neuron 2, weight 1 batch 2 grad: 0.182518

weight delta[1] = 0.704852

 neuron 2, weight 2 batch 0 grad: -0.755803

 neuron 2, weight 2 batch 1 grad: 0.210804

 neuron 2, weight 2 batch 2 grad: 0.266654

weight delta[2] = -0.278345

 neuron 2, weight 3 batch 0 grad: 0.172253

 neuron 2, weight 3 batch 1 grad: 0.074642

 neuron 2, weight 3 batch 2 grad: -0.269863

weight delta[3] = -0.022968

 neuron 2, weight 4 batch 0 grad: -0.589216

 neuron 2, weight 4 batch 1 grad: 0.053716

 neuron 2, weight 4 batch 2 grad: 0.315977

weight delta[4] = -0.219523

 neuron 3, weight 0 batch 0 grad: 0.070424

 neuron 3, weight 0 batch 1 grad: 0.045594

 neuron 3, weight 0 batch 2 grad: 0.022999

weight delta[0] = 0.139017

 neuron 3, weight 1 batch 0 grad: 0.069582

 neuron 3, weight 1 batch 1 grad: 0.042921

 neuron 3, weight 1 batch 2 grad: 0.009539

weight delta[1] = 0.122042

 neuron 3, weight 2 batch 0 grad: -0.070263

 neuron 3, weight 2 batch 1 grad: -0.040009

 neuron 3, weight 2 batch 2 grad: 0.013937

weight delta[2] = -0.096335

 neuron 3, weight 3 batch 0 grad: 0.016013

 neuron 3, weight 3 batch 1 grad: -0.014167

 neuron 3, weight 3 batch 2 grad: -0.014104

weight delta[3] = -0.012257

 neuron 3, weight 4 batch 0 grad: -0.054776

 neuron 3, weight 4 batch 1 grad: -0.010195

 neuron 3, weight 4 batch 2 grad: 0.016515

weight delta[4] = -0.048457

 0.109252  0.033443  -0.989530 

 total delta = -0.084684 

 bias [0] = -0.419639  delta = -0.028228 

 -0.939063  0.161626  0.517205 

 total delta = -0.054251 

 bias [1] = 1.500588  delta = -0.018084 

 0.759230  -0.240764  0.448866 

 total delta = 0.078649 

 bias [2] = 1.155838  delta = 0.026216 

 0.070582  0.045695  0.023460 

 total delta = 0.040190 

 bias [3] = -0.859374  delta = 0.013397 

embedding table:
token 0:	embedding: -1.216308 	 -0.678694 
token 1:	embedding: 0.158151 	 0.798470 
token 2:	embedding: 1.751835 	 0.296581 
token 3:	embedding: 1.155623 	 0.317850 

hidden layer: 
neuron 0, biase 0.734800
	Weight 0: -2.120812  	Weight 1: -0.044438  	Weight 2: 0.035260  	Weight 3: -1.247397  
neuron 1, biase 0.157078
	Weight 0: -0.974412  	Weight 1: -0.537589  	Weight 2: -0.582327  	Weight 3: 0.770801  
neuron 2, biase -0.014591
	Weight 0: 0.442207  	Weight 1: 1.227323  	Weight 2: 1.340377  	Weight 3: -0.180978  
neuron 3, biase -0.168911
	Weight 0: -0.023263  	Weight 1: 0.360482  	Weight 2: -0.865119  	Weight 3: -0.115292  
neuron 4, biase 0.573566
	Weight 0: -0.145824  	Weight 1: 1.486073  	Weight 2: 0.095179  	Weight 3: -0.748838  

output layer: 
neuron 0, bias -0.391411
	Weight 0: -0.551633  	Weight 1: 0.311229  	Weight 2: 0.381943  	Weight 3: 0.151558  	Weight 4: -1.289804  
neuron 1, bias 1.518672
	Weight 0: 0.199186  	Weight 1: -1.682310  	Weight 2: -0.804190  	Weight 3: -1.065959  	Weight 4: 1.408399  
neuron 2, bias 1.129622
	Weight 0: -0.513060  	Weight 1: 1.532319  	Weight 2: 0.538329  	Weight 3: -1.460933  	Weight 4: -0.685653  
neuron 3, bias -0.872771
	Weight 0: -0.219449  	Weight 1: 0.548186  	Weight 2: -0.141324  	Weight 3: 0.165725  	Weight 4: 0.163685  


logit 0: 0.027873 

logit 1: -0.283739 

logit 2: 2.131485 

logit 3: -0.542949 


correct index is 1

prob [0] = 0.095301

prob [1] = 0.069786

prob [2] = 0.781062

correct index is 6

prob [4] = 0.031174

prob [5] = 0.177214

prob [6] = 0.755459

correct index is 8

prob [8] = 0.010494

prob [9] = 0.619560

prob [10] = 0.350722

loss after back = 2.499890

epoch 3 


logit 0: 0.027873 

logit 1: -0.283739 

logit 2: 2.131485 

logit 3: -0.542949 

correct index is 1

prob [0] = 0.095301

prob [1] = 0.069786

prob [2] = 0.781062

correct index is 6

prob [4] = 0.031174

prob [5] = 0.177214

prob [6] = 0.755459

correct index is 8

prob [8] = 0.010494

prob [9] = 0.619560

prob [10] = 0.350722

loss before back = 2.499890

 backwards pass ...
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 0
offset_batch_embedding_element: 1
offset_batch_embedding_element: 8
offset_batch_embedding_element: 9
offset_batch_embedding_element: 10
offset_batch_embedding_element: 11
offset_batch_embedding_element: 18
offset_batch_embedding_element: 19
offset_batch_embedding_element: 20
offset_batch_embedding_element: 21

 0.095301  0.031174  -0.989506 

 total delta = -0.086303 

 bias [0] = -0.391411  delta = -0.028768 

 -0.930214  0.177214  0.619560 

 total delta = -0.042112 

 bias [1] = 1.518672  delta = -0.014037 

 0.781062  -0.244541  0.350722 

 total delta = 0.074687 

 bias [2] = 1.129622  delta = 0.024896 

 0.053851  0.036153  0.019224 

 total delta = 0.035818 

 bias [3] = -0.872771  delta = 0.011939 

 neuron 0, weight 0 batch 0 grad: 0.095064

 neuron 0, weight 0 batch 1 grad: 0.031098

 neuron 0, weight 0 batch 2 grad: -0.965280

weight delta[0] = -0.839118

 neuron 0, weight 1 batch 0 grad: 0.093718

 neuron 0, weight 1 batch 1 grad: 0.028802

 neuron 0, weight 1 batch 2 grad: -0.259398

weight delta[1] = -0.136877

 neuron 0, weight 2 batch 0 grad: -0.094786

 neuron 0, weight 2 batch 1 grad: -0.025731

 neuron 0, weight 2 batch 2 grad: -0.731858

weight delta[2] = -0.852374

 neuron 0, weight 3 batch 0 grad: 0.000770

 neuron 0, weight 3 batch 1 grad: -0.014942

 neuron 0, weight 3 batch 2 grad: 0.649749

weight delta[3] = 0.635576

 neuron 0, weight 4 batch 0 grad: -0.077002

 neuron 0, weight 4 batch 1 grad: -0.007418

 neuron 0, weight 4 batch 2 grad: -0.746683

weight delta[4] = -0.831103

 neuron 1, weight 0 batch 0 grad: -0.927902

 neuron 1, weight 0 batch 1 grad: 0.176778

 neuron 1, weight 0 batch 2 grad: 0.604392

weight delta[0] = -0.146732

 neuron 1, weight 1 batch 0 grad: -0.914766

 neuron 1, weight 1 batch 1 grad: 0.163728

 neuron 1, weight 1 batch 2 grad: 0.162417

weight delta[1] = -0.588621

 neuron 1, weight 2 batch 0 grad: 0.925182

 neuron 1, weight 2 batch 1 grad: -0.146268

 neuron 1, weight 2 batch 2 grad: 0.458239

weight delta[2] = 1.237153

 neuron 1, weight 3 batch 0 grad: -0.007514

 neuron 1, weight 3 batch 1 grad: -0.084941

 neuron 1, weight 3 batch 2 grad: -0.406828

weight delta[3] = -0.499283

 neuron 1, weight 4 batch 0 grad: 0.751595

 neuron 1, weight 4 batch 1 grad: -0.042168

 neuron 1, weight 4 batch 2 grad: 0.467521

weight delta[4] = 1.176948

 neuron 2, weight 0 batch 0 grad: 0.779120

 neuron 2, weight 0 batch 1 grad: -0.243940

 neuron 2, weight 0 batch 2 grad: 0.342135

weight delta[0] = 0.877316

 neuron 2, weight 1 batch 0 grad: 0.768091

 neuron 2, weight 1 batch 1 grad: -0.225932

 neuron 2, weight 1 batch 2 grad: 0.091941

weight delta[1] = 0.634100

 neuron 2, weight 2 batch 0 grad: -0.776836

 neuron 2, weight 2 batch 1 grad: 0.201838

 neuron 2, weight 2 batch 2 grad: 0.259401

weight delta[2] = -0.315597

 neuron 2, weight 3 batch 0 grad: 0.006309

 neuron 2, weight 3 batch 1 grad: 0.117212

 neuron 2, weight 3 batch 2 grad: -0.230298

weight delta[3] = -0.106777

 neuron 2, weight 4 batch 0 grad: -0.631083

 neuron 2, weight 4 batch 1 grad: 0.058189

 neuron 2, weight 4 batch 2 grad: 0.264656

weight delta[4] = -0.308239

 neuron 3, weight 0 batch 0 grad: 0.053717

 neuron 3, weight 0 batch 1 grad: 0.036064

 neuron 3, weight 0 batch 2 grad: 0.018753

weight delta[0] = 0.108534

 neuron 3, weight 1 batch 0 grad: 0.052957

 neuron 3, weight 1 batch 1 grad: 0.033402

 neuron 3, weight 1 batch 2 grad: 0.005039

weight delta[1] = 0.091398

 neuron 3, weight 2 batch 0 grad: -0.053560

 neuron 3, weight 2 batch 1 grad: -0.029840

 neuron 3, weight 2 batch 2 grad: 0.014218

weight delta[2] = -0.069182

 neuron 3, weight 3 batch 0 grad: 0.000435

 neuron 3, weight 3 batch 1 grad: -0.017329

 neuron 3, weight 3 batch 2 grad: -0.012623

weight delta[3] = -0.029517

 neuron 3, weight 4 batch 0 grad: -0.043511

 neuron 3, weight 4 batch 1 grad: -0.008603

 neuron 3, weight 4 batch 2 grad: 0.014506

weight delta[4] = -0.037607

 0.095301  0.031174  -0.989506 

 total delta = -0.086303 

 bias [0] = -0.362644  delta = -0.028768 

 -0.930214  0.177214  0.619560 

 total delta = -0.042112 

 bias [1] = 1.532709  delta = -0.014037 

 0.781062  -0.244541  0.350722 

 total delta = 0.074687 

 bias [2] = 1.104726  delta = 0.024896 

 0.053851  0.036153  0.019224 

 total delta = 0.035818 

 bias [3] = -0.884710  delta = 0.011939 

embedding table:
token 0:	embedding: -1.175307 	 -0.588140 
token 1:	embedding: 0.308662 	 0.950241 
token 2:	embedding: 1.988187 	 0.447091 
token 3:	embedding: 1.306133 	 0.469620 

hidden layer: 
neuron 0, biase 0.734800
	Weight 0: -2.119318  	Weight 1: -0.044381  	Weight 2: 0.034059  	Weight 3: -1.247397  
neuron 1, biase 0.157078
	Weight 0: -0.967140  	Weight 1: -0.532941  	Weight 2: -0.582354  	Weight 3: 0.770801  
neuron 2, biase -0.014591
	Weight 0: 0.455568  	Weight 1: 1.234972  	Weight 2: 1.338941  	Weight 3: -0.180978  
neuron 3, biase -0.168911
	Weight 0: 0.048507  	Weight 1: 0.418449  	Weight 2: -0.846558  	Weight 3: -0.115292  
neuron 4, biase 0.573566
	Weight 0: -0.093501  	Weight 1: 1.521490  	Weight 2: 0.098060  	Weight 3: -0.748838  

output layer: 
neuron 0, bias -0.333876
	Weight 0: -0.523662  	Weight 1: 0.315792  	Weight 2: 0.410355  	Weight 3: 0.130373  	Weight 4: -1.262100  
neuron 1, bias 1.546746
	Weight 0: 0.204077  	Weight 1: -1.662689  	Weight 2: -0.845428  	Weight 3: -1.049316  	Weight 4: 1.369167  
neuron 2, bias 1.079830
	Weight 0: -0.542304  	Weight 1: 1.511182  	Weight 2: 0.548849  	Weight 3: -1.457374  	Weight 4: -0.675379  
neuron 3, bias -0.896650
	Weight 0: -0.223067  	Weight 1: 0.545139  	Weight 2: -0.139018  	Weight 3: 0.166709  	Weight 4: 0.164939  


logit 0: 0.054795 

logit 1: 0.064030 

logit 2: 2.343574 

logit 3: -0.619251 


correct index is 1

prob [0] = 0.080764

prob [1] = 0.081513

prob [2] = 0.796563

correct index is 6

prob [4] = 0.028451

prob [5] = 0.207184

prob [6] = 0.734567

correct index is 8

prob [8] = 0.008814

prob [9] = 0.776272

prob [10] = 0.201683

 new loss is greater: 2.515642
